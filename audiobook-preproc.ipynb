{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "greenhouse-mount",
   "metadata": {},
   "source": [
    "# **Audiobook data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "weighted-change",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Audiobooks_data.csv',\n",
       " 'audiobook_train.npz',\n",
       " '.ipynb_checkpoints',\n",
       " 'Untitled.ipynb',\n",
       " 'audiobook-preproc.ipynb',\n",
       " 'audiobook_val.npz',\n",
       " 'audiobook_test.npz']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "quarterly-approval",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-amino",
   "metadata": {},
   "source": [
    "This data represents **2 years** of engagement. We are going to do supervised learning and the target is the last column.<br>\n",
    "This target is expressed as boolean, during the **next 6 month** after the data gathering, we checked if the person purchased another book (i.e. *1* if *yes*, *0* if *no*). <br>\n",
    "In other words we have a **classification problem**, will buy or no. The audiobook company wants to do targeted marketing, they want to know if a customer is *likely* to make new purchase or no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "streaming-slide",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Book_length_overall</th>\n",
       "      <th>Book_length_avg</th>\n",
       "      <th>Price_avg</th>\n",
       "      <th>Price_overall</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_over_10</th>\n",
       "      <th>Minutes_listened</th>\n",
       "      <th>Completion</th>\n",
       "      <th>Support_requests</th>\n",
       "      <th>Last_visit</th>\n",
       "      <th>Targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>873</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>2160</td>\n",
       "      <td>10.13</td>\n",
       "      <td>10.13</td>\n",
       "      <td>0</td>\n",
       "      <td>8.91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>611</td>\n",
       "      <td>1404.0</td>\n",
       "      <td>2808</td>\n",
       "      <td>6.66</td>\n",
       "      <td>13.33</td>\n",
       "      <td>1</td>\n",
       "      <td>6.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>182</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>705</td>\n",
       "      <td>324.0</td>\n",
       "      <td>324</td>\n",
       "      <td>10.13</td>\n",
       "      <td>10.13</td>\n",
       "      <td>1</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>334</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>391</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>1620</td>\n",
       "      <td>15.31</td>\n",
       "      <td>15.31</td>\n",
       "      <td>0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>183</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>819</td>\n",
       "      <td>432.0</td>\n",
       "      <td>1296</td>\n",
       "      <td>7.11</td>\n",
       "      <td>21.33</td>\n",
       "      <td>1</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID  Book_length_overall  Book_length_avg  Price_avg  Price_overall  \\\n",
       "0  873               2160.0             2160      10.13          10.13   \n",
       "1  611               1404.0             2808       6.66          13.33   \n",
       "2  705                324.0              324      10.13          10.13   \n",
       "3  391               1620.0             1620      15.31          15.31   \n",
       "4  819                432.0             1296       7.11          21.33   \n",
       "\n",
       "   Review  Review_over_10  Minutes_listened  Completion  Support_requests  \\\n",
       "0       0            8.91               0.0         0.0                 0   \n",
       "1       1            6.50               0.0         0.0                 0   \n",
       "2       1            9.00               0.0         0.0                 1   \n",
       "3       0            9.00               0.0         0.0                 0   \n",
       "4       1            9.00               0.0         0.0                 0   \n",
       "\n",
       "   Last_visit  Targets  \n",
       "0           0        1  \n",
       "1         182        1  \n",
       "2         334        1  \n",
       "3         183        1  \n",
       "4           0        1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names = [\n",
    "    'ID',   \n",
    "    'Book_length_overall', # Total of length of all books in minutes\n",
    "    'Book_length_avg', # Total of length of all books / Number of books in minutes\n",
    "    'Price_avg', \n",
    "    'Price_overall',\n",
    "    'Review', # Number of review\n",
    "    'Review_over_10',  # Average review over 10\n",
    "    'Minutes_listened',\n",
    "    'Completion', # Pct of minute read over total book length\n",
    "    'Support_requests', # Number of support requested\n",
    "    'Last_visit', # Last visit since purchase in days\n",
    "    'Targets',\n",
    "]\n",
    "\n",
    "ab = pd.read_csv('Audiobooks_data.csv', names=col_names)\n",
    "ab.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-belief",
   "metadata": {},
   "source": [
    "## **Preprocessing**\n",
    "- First task *Balance the dataset*\n",
    "- Divide into training, validation and test subsets\n",
    "- Prepare the data for tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cooked-netscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "raw_csv_data = np.loadtxt('Audiobooks_data.csv', delimiter=',')\n",
    "unscaled_inputs_all = raw_csv_data[:,1:-1]\n",
    "targets_all = raw_csv_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-money",
   "metadata": {},
   "source": [
    "**Balancing the dataset**<br>\n",
    "Balance the outputs (here 0 and 1) to make the model work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "reserved-affair",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ones: 2237.0\n",
      "Number of zeros: 11847.0\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of ones: {raw_csv_data[:,-1].sum()}')\n",
    "print(f'Number of zeros: {raw_csv_data[:,-1].shape[0] - raw_csv_data[:,-1].sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-facial",
   "metadata": {},
   "source": [
    "As we can see, the number of *ones* and *zeros* are inbalanced. That may lead the model to tend to classify our prediciton more to zero that to one. <br>\n",
    "Therefore, we need to balance it as *50 %* for each class. <br>\n",
    "But this step cripples us as we have to remove a big amount of data, one way to correct this is to create a loop for balancing, each loop works with different portion of observation with zeros as target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "prerequisite-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_one = int(targets_all.sum())\n",
    "zero_counter = 0\n",
    "indices_to_remove = []\n",
    "\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i] == 0: \n",
    "        zero_counter += 1 # Counts the 0 in the targets\n",
    "        # When the count reaches the same length as the count of ones, it adds the indices to the list\n",
    "        if zero_counter > num_one:  \n",
    "            indices_to_remove.append(i)\n",
    "            \n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0)\n",
    "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-private",
   "metadata": {},
   "source": [
    "**Standardize the inputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "informal-moore",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-specification",
   "metadata": {},
   "source": [
    "**Shuffle the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "western-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = np.arange(scaled_inputs.shape[0]) # get the indices for the inputs\n",
    "np.random.shuffle(shuffled_indices) # Shuffling the indices for good batching\n",
    "\n",
    "# Shuffled dataset\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-saver",
   "metadata": {},
   "source": [
    "**Split the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sexual-debate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of ones in the training target set: 50.6%\n",
      "% of ones in the validation target set: 49.3%\n",
      "% of ones in the test target set: 47.4%\n"
     ]
    }
   ],
   "source": [
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "# We will use the 70-20-10 split\n",
    "train_count = int(.7*samples_count)\n",
    "val_count = int(.2*samples_count)\n",
    "test_count = samples_count - train_count - val_count\n",
    "\n",
    "# Splitting the dataset\n",
    "train_inputs = shuffled_inputs[:train_count]\n",
    "train_targets = shuffled_targets[:train_count]\n",
    "print(f'% of ones in the training target set: {train_targets.sum()/train_count:.1%}')\n",
    "\n",
    "val_inputs = shuffled_inputs[train_count:train_count+val_count]\n",
    "val_targets = shuffled_targets[train_count:train_count+val_count]\n",
    "print(f'% of ones in the validation target set: {val_targets.sum()/val_count:.1%}')\n",
    "\n",
    "test_inputs = shuffled_inputs[train_count+val_count:]\n",
    "test_targets = shuffled_targets[train_count+val_count:]\n",
    "print(f'% of ones in the test target set: {test_targets.sum()/test_count:.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-clerk",
   "metadata": {},
   "source": [
    "Sets are balanced, we are good to go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-regard",
   "metadata": {},
   "source": [
    "**Save the data in npz.file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "mechanical-cemetery",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('audiobook_train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('audiobook_val', inputs=val_inputs, targets=val_targets)\n",
    "np.savez('audiobook_test', inputs=test_inputs, targets=test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-class",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
